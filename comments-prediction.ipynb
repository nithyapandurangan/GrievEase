{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-12T14:50:09.975799Z","iopub.execute_input":"2023-04-12T14:50:09.977028Z","iopub.status.idle":"2023-04-12T14:50:10.023773Z","shell.execute_reply.started":"2023-04-12T14:50:09.976971Z","shell.execute_reply":"2023-04-12T14:50:10.021545Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/mental-health-corpus/mental_health.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Importing Packages","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nimport spacy\nimport nltk \nimport string\nimport regex as re \nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import svm\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras.layers import TextVectorization\nimport tensorflow as tf \nfrom tensorflow.keras.layers import LSTM, Dropout, Bidirectional, Dense, Embedding, BatchNormalization\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy","metadata":{"execution":{"iopub.status.busy":"2023-04-12T14:50:46.035213Z","iopub.execute_input":"2023-04-12T14:50:46.036579Z","iopub.status.idle":"2023-04-12T14:51:05.824580Z","shell.execute_reply.started":"2023-04-12T14:50:46.036530Z","shell.execute_reply":"2023-04-12T14:51:05.822736Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/mental-health-corpus/mental_health.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-12T14:51:08.339313Z","iopub.execute_input":"2023-04-12T14:51:08.340352Z","iopub.status.idle":"2023-04-12T14:51:08.729576Z","shell.execute_reply.started":"2023-04-12T14:51:08.340297Z","shell.execute_reply":"2023-04-12T14:51:08.728123Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Modules to remove the stopwords","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_lg')\nsp = spacy.load('en_core_web_lg')\n\nnltk.download('stopwords')\nnltk.download('punkt')\n\nspacy_st = nlp.Defaults.stop_words\nnltk_st = stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2023-04-12T14:51:30.932208Z","iopub.execute_input":"2023-04-12T14:51:30.932702Z","iopub.status.idle":"2023-04-12T14:52:19.009174Z","shell.execute_reply.started":"2023-04-12T14:51:30.932662Z","shell.execute_reply":"2023-04-12T14:52:19.007236Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n[nltk_data]     Temporary failure in name resolution>\n[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n[nltk_data]     failure in name resolution>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Function to clean the text","metadata":{}},{"cell_type":"code","source":"def clean(text, http=True, punc=True, lem=True, stop_w=True):\n    if http == True:\n        text = re.sub('https?:\\/\\/t.co\\/[A-Za-z0-9]*', '', text)\n    if stop_w == True:\n        text = [word for word in word_tokenize(text) if not word.lower() in nltk_st]\n        text = ' '.join(text)\n    if lem == True:\n        lemmatized = [word.lemma_ for word in sp(text)]\n        text = ' '.join(lemmatized)\n    if punc == True:\n        text = text.translate(str.maketrans('', '', string.punctuation))\n        \n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"im\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('a{2,}', '', text)\n    text = re.sub('b{2,}', '', text)\n    text = re.sub('c{2,}', '', text)\n    text = re.sub('d{2,}', '', text)\n    text = re.sub('e{2,}', '', text)\n    text = re.sub('f{2,}', '', text)\n    text = re.sub('g{2,}', '', text)\n    text = re.sub('h{2,}', '', text)\n    text = re.sub('i{2,}', '', text)\n    text = re.sub('j{2,}', '', text)\n    text = re.sub('k{2,}', '', text)\n    text = re.sub('l{2,}', '', text)\n    text = re.sub('m{2,}', '', text)\n    text = re.sub('n{2,}', '', text)\n    text = re.sub('o{2,}', '', text)\n    text = re.sub('p{2,}', '', text)\n    text = re.sub('q{2,}', '', text)\n    text = re.sub('r{2,}', '', text)\n    text = re.sub('s{2,}', '', text)\n    text = re.sub('t{2,}', '', text)\n    text = re.sub('u{2,}', '', text)\n    text = re.sub('v{2,}', '', text)\n    text = re.sub('w{2,}', '', text)\n    text = re.sub('x{2,}', '', text)\n    text = re.sub('y{2,}', '', text)\n    text = re.sub('z{2,}', '', text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip(' ')\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2023-04-12T14:52:49.875775Z","iopub.execute_input":"2023-04-12T14:52:49.876285Z","iopub.status.idle":"2023-04-12T14:52:49.897502Z","shell.execute_reply.started":"2023-04-12T14:52:49.876246Z","shell.execute_reply":"2023-04-12T14:52:49.895800Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train['cleaned_text'] = train['text'].apply(lambda text: clean(text, http=True, punc=True, lem=True, stop_w=True))","metadata":{"execution":{"iopub.status.busy":"2023-04-12T14:52:59.993168Z","iopub.execute_input":"2023-04-12T14:52:59.993733Z","iopub.status.idle":"2023-04-12T15:02:52.954034Z","shell.execute_reply.started":"2023-04-12T14:52:59.993681Z","shell.execute_reply":"2023-04-12T15:02:52.952258Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train.drop(columns=['text'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T15:02:52.956697Z","iopub.execute_input":"2023-04-12T15:02:52.957391Z","iopub.status.idle":"2023-04-12T15:02:52.977796Z","shell.execute_reply.started":"2023-04-12T15:02:52.957344Z","shell.execute_reply":"2023-04-12T15:02:52.976176Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X = train['cleaned_text']\ny = train['label']\n\nMAX_FEATURES = 200000\n\nvectorizer = TextVectorization(\n    max_tokens=MAX_FEATURES, \n    output_sequence_length=1000, \n    output_mode='int'\n)\n\nvectorizer.adapt(X.values)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T15:02:52.979627Z","iopub.execute_input":"2023-04-12T15:02:52.980102Z","iopub.status.idle":"2023-04-12T15:02:56.040686Z","shell.execute_reply.started":"2023-04-12T15:02:52.980048Z","shell.execute_reply":"2023-04-12T15:02:56.039225Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"vectorizerd_text = vectorizer(X.values)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T15:02:56.044989Z","iopub.execute_input":"2023-04-12T15:02:56.045692Z","iopub.status.idle":"2023-04-12T15:02:56.858447Z","shell.execute_reply.started":"2023-04-12T15:02:56.045637Z","shell.execute_reply":"2023-04-12T15:02:56.856948Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices((vectorizerd_text, y))\ndataset = dataset.cache()\ndataset = dataset.shuffle(160000)\ndataset = dataset.batch(32) \ndataset = dataset.prefetch(8)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T15:02:56.861708Z","iopub.execute_input":"2023-04-12T15:02:56.862195Z","iopub.status.idle":"2023-04-12T15:02:56.892296Z","shell.execute_reply.started":"2023-04-12T15:02:56.862157Z","shell.execute_reply":"2023-04-12T15:02:56.890640Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"batch_X, batch_y = dataset.as_numpy_iterator().next()\nbatch_X.shape, batch_y.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-12T15:02:56.894557Z","iopub.execute_input":"2023-04-12T15:02:56.895004Z","iopub.status.idle":"2023-04-12T15:02:57.225286Z","shell.execute_reply.started":"2023-04-12T15:02:56.894969Z","shell.execute_reply":"2023-04-12T15:02:57.224309Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"((32, 1000), (32,))"},"metadata":{}}]},{"cell_type":"code","source":"train = dataset.take(int(len(dataset)*.7))\nval = dataset.skip(int(len(dataset)*.7)).take(int(len(dataset)*.2))\ntest = dataset.skip(int(len(dataset)*.9)).take(int(len(dataset)*.1))\nlen(train), len(val), len(test)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T15:02:57.226430Z","iopub.execute_input":"2023-04-12T15:02:57.227189Z","iopub.status.idle":"2023-04-12T15:02:57.244022Z","shell.execute_reply.started":"2023-04-12T15:02:57.227149Z","shell.execute_reply":"2023-04-12T15:02:57.242749Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(612, 175, 87)"},"metadata":{}}]},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Embedding(MAX_FEATURES + 1, 32))\nmodel.add(Bidirectional(LSTM(32, activation='tanh')))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='BinaryCrossentropy', optimizer='Adam', metrics=['accuracy'])\nmodel.summary()\nhist = model.fit(train, epochs=15, batch_size=32, validation_data=val)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T15:02:57.246050Z","iopub.execute_input":"2023-04-12T15:02:57.247124Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, None, 32)          6400032   \n                                                                 \n bidirectional (Bidirectiona  (None, 64)               16640     \n l)                                                              \n                                                                 \n dense (Dense)               (None, 128)               8320      \n                                                                 \n batch_normalization (BatchN  (None, 128)              512       \n ormalization)                                                   \n                                                                 \n dropout (Dropout)           (None, 128)               0         \n                                                                 \n dense_1 (Dense)             (None, 256)               33024     \n                                                                 \n batch_normalization_1 (Batc  (None, 256)              1024      \n hNormalization)                                                 \n                                                                 \n dropout_1 (Dropout)         (None, 256)               0         \n                                                                 \n dense_2 (Dense)             (None, 128)               32896     \n                                                                 \n batch_normalization_2 (Batc  (None, 128)              512       \n hNormalization)                                                 \n                                                                 \n dropout_2 (Dropout)         (None, 128)               0         \n                                                                 \n dense_3 (Dense)             (None, 64)                8256      \n                                                                 \n batch_normalization_3 (Batc  (None, 64)               256       \n hNormalization)                                                 \n                                                                 \n dropout_3 (Dropout)         (None, 64)                0         \n                                                                 \n dense_4 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 6,501,537\nTrainable params: 6,500,385\nNon-trainable params: 1,152\n_________________________________________________________________\nEpoch 1/15\n612/612 [==============================] - 426s 684ms/step - loss: 0.3567 - accuracy: 0.8405 - val_loss: 0.2418 - val_accuracy: 0.9180\nEpoch 2/15\n572/612 [===========================>..] - ETA: 25s - loss: 0.2182 - accuracy: 0.9136","output_type":"stream"}]},{"cell_type":"code","source":"pre = Precision()\nre = Recall()\nacc = CategoricalAccuracy()\n\nfor batch in test.as_numpy_iterator():\n    X_true, y_true = batch\n    yhat = model.predict(X_true)\n    \n    y_preds = []\n    for y in yhat:\n        if y >= 0.5:\n            y_preds.append(1)\n        else:\n            y_preds.append(0)\n    \n    y_true = y_true.flatten()\n    \n    pre.update_state(y_true, y_preds)\n    re.update_state(y_true, y_preds)\n    acc.update_state(y_true, y_preds)\n\nprint(f'Precision: {pre.result().numpy()}, Recall: {re.result().numpy()}, Accuracy: {acc.result().numpy()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}